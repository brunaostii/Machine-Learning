- Como a normalização afetou o resultado?
    - Em especial para o método do K-means, quando temos variáveis com unidades que não são compatíveis - por exemplo, uma medida de massa em quilos e uma de altura em centímetros -, existe a necessidade de ajustar as variáveis. Além disso, mesmo que as medidas sejam de unidades compatíveis, se a variância entre elas for considerável, ainda assim é interessante que os dados sejam normalizados. Isto pois o método do K-means tende a produzir clusters que apresentam um formato relativamente redondo. Portanto, se deixamos a variância das variáveis desigual, o que acontece é que colocamos um peso maior nas variáveis com menor variância, fazendo com que os clusters se acumulem ao redor das variáveis com maior variância.
    Tanto para o dataset 1, quanto para o dataset 2, vemos que quando aplicamos o K-means para estes datasets normalizados, o Elbow Method nos forneceu um número ideal de clusters mais claro. O mesmo pode ser visto para o Elbow Method aplicado ao dataset 2; a definição do número de clusters ideal é mais clara.
    Além disso, ao analisarmos os clusters do dataset 1 não-normalizados, vemos que alguns pontos que deveriam ser do cluster 2 foram considerados como pontos do cluster 0; já quando este dataset 1 foi normalizado, o comportamento dos clusters é o esperado.
- A quantidade de clusters é representativo pros dados?
    - Ao olharmos para o gráfico dos dados do dataset 1 sem qualquer aplicação de clusterização, fica claro que deveriam existir 3 clusters diferentes. Validamos esta suspeita através da normalização dos dados e, a partir disso, da aplicação do Elbow Method para descobrir o número ideal de clusters. Se analisarmos este gráfico do Elbow Method para o dataset 1 normalizado, confirmamos a ideia de que o número ideal de clusters é de 3 - além disso, o gráfico do K-means com K = 3 para o dataset 1 normalizado deixa claro que este é o número ideal de clusters.
- A inicialização dos clusters afetou o resultado?
    - Durante o processo de escolha do método de inicialização, notamos que o K-means sofre de alta sensibilidade em relação ao local em que os clusters são inicializados. A princípio, decidimos por uma inicialização randômica - escolhíamos K pontos aleatoriamente dentro do dataset para serem os centros dos clusters, sem qualquer tipo de checagem sobre a posição relativa destes centros. Este tipo de inicialização pode afetar de forma negativa a formação dos clusters; por exemplo, se dois centros de clusters forem inicializados muito próximos um ao outro ou, ainda, se todos os centros dos clusters se encontrarem na mesma região, a qualidade do algoritmo será afetada.
    Portanto, decidimos por implementar o K-means++, que é um algoritmo de inicialização de clusters que consiste em escolher o primeiro centro aleatoriamente e, para os outros K - 1 centros de clusters, computar a distância de todos os pontos para os centros já existentes e selecionar um dos pontos mais distantes para ser o próximo centro.
    Por fim, notamos que a consistência das iterações do K-means aumentou com a implementação do K-means++; já que garantimos uma distância entre os centros dos clusters já na inicialização, facilitamos o processo do algoritmo do K-means.

- Vantagens e desvantagens de cada método
    - Em primeiro lugar, o número de operações do K-means é muito menor do que o do Mean Shift; são $O(kN)$ operações para o K-means e $O(kN^2)$ para o Mean Shift, em que $N$ é o número de pontos da base de dados e $k$ representa o número de iterações realizadas em cada ponto. Portanto, o Mean Shift é um algoritmo custoso, enquanto que o K-means pode ser aplicado para datasets de tamanhos maiores sem grandes perdas de desempenho. O Mean Shift precisa de uma base que contenha alta densidade de dados que possam ser rotulados de forma clara; o Mean Shift sofre, portanto, quando temos outliers presentes na base. Por outro lado, o K-means precisa que o número de clusters seja pré-determinado - e esta pode ser uma tarefa difícil de ser realizada; além disso, o K-means se comporta melhor quando o formato dos clusters é convexo, sofrendo perdas quando esta condição não é cumprida.
    O K-means, portanto, é um método simples e de execução rápida com fácil implementação, possibilitando que seja adaptado sem grandes dificuldades - temos como exemplo a possibilidade de alterar a medida de distância utilizada pelo algoritmo. Sofre, porém, pela alta sensibilidade à escala dos dados, pela necessidade de se definir o número de clusters e, também, por não funcionar com qualquer formato de clusters.
    O Mean Shift, por outro lado, não necessita de que seus clusters sejam pré-determinados e funciona para um número arbitrário de features e, também, para formatos de clusters que não são contemplados pelo K-means.
- Como a dimensionalidade afetou os resultados?
    - Em primeiro lugar, o dataset 2 possui mais dimensões do que podemos analisar em gráficos; portanto, a dimensionalidade desta base de dados impede que seja possível analisar suas propriedades sem algum tipo de tratamento.
    Para isso usamos de métodos de redução da dimensionalidade - estes métodos servem para reduzir o número de dimensões de uma base de dados de forma que propriedades significativas sejam mantidas e possam ser analisadas. Neste trabalho, aplicamos o PCA para este fim; ao reduzir a dimensionalidade, escolhemos como parâmetro para a redução valores entre 1 e 3 dimensões. Isto nos possibilitou executar os algoritmos de clusterização e analisar de que forma este banco de dados se comporta quando temos apenas as features mais relevantes evidenciadas. Aplicamos o K-means com valores de K variando entre 3 e 5 para estas três instâncias do PCA. 
    Portanto, esta redução de dimensionalidades permite a visualização dos dados e aplicação de algoritmos que nos ajudem a entender de que forma o banco de dados se comporta. Auxilia, também, com que os algoritmos de clusterização possam convergir com maior facilidade e, dependendo do tipo de dados que existem na base, também proporciona maior nitidez na separação de classes.